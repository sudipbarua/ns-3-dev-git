{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s break down the **code implementation** of **MO-PPO for ADR** in a LoRaWAN network with NS3.\n",
    "\n",
    "### **1. Install Dependencies**\n",
    "Ensure you have the necessary dependencies installed, especially for **neural networks** and **reinforcement learning**.\n",
    "\n",
    "- **PyTorch**: For implementing the neural networks (actor and critic).\n",
    "- **Gym** (optional): For defining the environment in case you need it.\n",
    "- **NS3**: For simulating the LoRaWAN network (you likely already have this).\n",
    "\n",
    "Hereâ€™s how you can install the required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install torch gym numpy matplotlib\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Defining the PPO Model**\n",
    "\n",
    "Hereâ€™s a basic implementation of the **actor** and **critic** models using **PyTorch**.\n",
    "\n",
    "#### **Actor (Policy Network)**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        action_probs = torch.softmax(self.fc3(x), dim=-1)\n",
    "        return action_probs\n",
    "```\n",
    "\n",
    "#### **Critic (Value Network)**\n",
    "\n",
    "```python\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)  # Single value output for state value\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        state_value = self.fc3(x)\n",
    "        return state_value\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. PPO Loss Function**\n",
    "\n",
    "PPO uses a clipped objective for stable training.\n",
    "\n",
    "```python\n",
    "def ppo_loss(actor, critic, states, actions, rewards, old_probs, gamma=0.99, epsilon=0.2):\n",
    "    # Calculate advantage estimates (using rewards and critic)\n",
    "    state_values = critic(states)\n",
    "    advantages = rewards - state_values.detach()\n",
    "\n",
    "    # Calculate new action probabilities\n",
    "    new_probs = actor(states)\n",
    "    action_prob = new_probs.gather(1, actions.unsqueeze(-1))\n",
    "    old_prob = old_probs.gather(1, actions.unsqueeze(-1))\n",
    "\n",
    "    # Calculate the ratio\n",
    "    ratio = action_prob / old_prob\n",
    "    surrogate_loss = ratio * advantages\n",
    "    clipped_loss = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "\n",
    "    # Final PPO loss\n",
    "    loss = -torch.min(surrogate_loss, clipped_loss).mean()  # Negative for max optimization\n",
    "    return loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Training the PPO Agent**\n",
    "\n",
    "Next, youâ€™ll need to implement the training loop. The agent interacts with NS3 and trains using the **PPO loss**.\n",
    "\n",
    "```python\n",
    "def train_ppo(actor, critic, optimizer_actor, optimizer_critic, episodes=1000, gamma=0.99):\n",
    "    for episode in range(episodes):\n",
    "        # Initialize state (from NS3 LoRaWAN simulation)\n",
    "        state = get_initial_state()  # NS3 will provide this\n",
    "\n",
    "        # Track episode experience\n",
    "        states, actions, rewards, old_probs = [], [], [], []\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Choose action from the actor network\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action_probs = actor(state_tensor)\n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            # Store the action and probability\n",
    "            states.append(state_tensor)\n",
    "            actions.append(action)\n",
    "            old_probs.append(action_probs[action].item())\n",
    "\n",
    "            # Take action in NS3 and get new state and reward\n",
    "            next_state, reward, done = simulate_step(state, action)  # Simulate in NS3\n",
    "\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.tensor(actions)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        old_probs = torch.tensor(old_probs)\n",
    "\n",
    "        # Calculate PPO loss\n",
    "        loss = ppo_loss(actor, critic, states, actions, rewards, old_probs)\n",
    "\n",
    "        # Update actor and critic\n",
    "        optimizer_actor.zero_grad()\n",
    "        optimizer_critic.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_actor.step()\n",
    "        optimizer_critic.step()\n",
    "\n",
    "        # Print loss every few episodes\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Loss: {loss.item()}\")\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. NS3 Simulation Integration**\n",
    "\n",
    "Now, we need to **integrate the RL agent with NS3**. Below is a rough sketch for how this might work in an NS3 Python API (if available) or a custom wrapper around C++.\n",
    "\n",
    "#### **simulate_step** function\n",
    "Youâ€™ll need a function to interact with the NS3 simulation. This will send the ADR decision (SF and TP) and receive the reward.\n",
    "\n",
    "```python\n",
    "def simulate_step(state, action):\n",
    "    # Map action to SF/TP choice\n",
    "    sf, tp = map_action_to_sf_tp(action)\n",
    "\n",
    "    # Set the ADR parameters in NS3\n",
    "    set_sf_tp_in_ns3(sf, tp)\n",
    "\n",
    "    # Run the simulation and get the results\n",
    "    pdr, energy_consumption = run_ns3_simulation()  # Return PDR and energy consumption\n",
    "\n",
    "    # Reward is calculated as a combination of PDR and energy efficiency\n",
    "    reward = calculate_reward(pdr, energy_consumption)\n",
    "    return get_new_state(), reward, is_done()\n",
    "```\n",
    "\n",
    "### **6. Running the Training**\n",
    "Once everything is set up, you can run the training loop.\n",
    "\n",
    "```python\n",
    "# Initialize the networks and optimizers\n",
    "state_dim = 10  # Number of state variables (for example)\n",
    "action_dim = 5  # Number of possible action choices (SF, TP)\n",
    "\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=0.0003)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=0.0003)\n",
    "\n",
    "# Train the agent\n",
    "train_ppo(actor, critic, optimizer_actor, optimizer_critic)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps and Customization**\n",
    "1. **Fine-tune Hyperparameters**:\n",
    "   - Adjust learning rates, batch sizes, and other hyperparameters for best performance.\n",
    "2. **Connect to Real NS3 LoRaWAN Setup**:\n",
    "   - Ensure the `simulate_step` function is fully integrated with your NS3 environment.\n",
    "3. **Evaluation**:\n",
    "   - After training, evaluate the model in various network conditions.\n",
    "4. **Advanced Exploration**:\n",
    "   - You can extend the training by using **MO-PPO with entropy** to improve exploration.\n",
    "\n",
    "---\n",
    "\n",
    "### **Would you like further assistance with any part of the code, such as setting up the NS3 wrapper or hyperparameter tuning?** Let me know! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect the **NS3 simulator** with your **Python trainer** using **socket programming**, you can create a communication bridge between NS3 and Python. This allows you to run the NS3 simulation and interact with it in real-time, sending actions (like SF and TP values) and receiving rewards (like PDR and energy consumption).\n",
    "\n",
    "Hereâ€™s a step-by-step guide on how to achieve this using **sockets**:\n",
    "\n",
    "### **1. NS3 Side (C++ Code)**\n",
    "You will write a **server** in NS3 that listens for connections from the Python trainer. When it receives an action (SF and TP), it will run the simulation step and send back the reward (PDR and energy consumption).\n",
    "\n",
    "#### **NS3 C++ Server Example**\n",
    "In NS3, you can use the **socket API** to create a server that listens for connections from the Python side. This example assumes you are familiar with NS3 and have a working LoRaWAN simulation.\n",
    "\n",
    "```cpp\n",
    "#include \"ns3/core-module.h\"\n",
    "#include \"ns3/network-module.h\"\n",
    "#include \"ns3/internet-module.h\"\n",
    "#include \"ns3/socket.h\"\n",
    "#include \"ns3/address.h\"\n",
    "#include \"ns3/udp-socket.h\"\n",
    "#include \"ns3/packet.h\"\n",
    "#include <sstream>\n",
    "\n",
    "using namespace ns3;\n",
    "\n",
    "class LoRaWANServer {\n",
    "public:\n",
    "    void RunServer(uint16_t port) {\n",
    "        // Create a socket to listen for connections\n",
    "        Ptr<Socket> socket = Socket::CreateSocket (GetNode(), TypeId::LookupByName(\"ns3::UdpSocketFactory\"));\n",
    "        InetSocketAddress local = InetSocketAddress (Ipv4Address::GetAny(), port);\n",
    "        socket->Bind(local);\n",
    "        socket->SetRecvCallback(MakeCallback(&LoRaWANServer::HandleRead, this));\n",
    "        \n",
    "        Simulator::Run ();\n",
    "        Simulator::Destroy ();\n",
    "    }\n",
    "\n",
    "    void HandleRead(Ptr<Socket> socket) {\n",
    "        Address from;\n",
    "        Ptr<Packet> packet = socket->RecvFrom(from);\n",
    "        \n",
    "        // Read the action (SF, TP) from the packet\n",
    "        uint32_t action;\n",
    "        packet->RemoveAtStart(sizeof(action));\n",
    "        \n",
    "        // Simulate the LoRaWAN behavior based on the action (SF, TP)\n",
    "        float pdr = SimulateLoRaWAN(action);\n",
    "        float energyConsumption = CalculateEnergyConsumption(action);\n",
    "        \n",
    "        // Send the reward (PDR and energy consumption) back to the trainer\n",
    "        std::ostringstream response;\n",
    "        response << pdr << \",\" << energyConsumption;\n",
    "        socket->SendTo(MakeShared<Packet>((uint8_t*)response.str().c_str(), response.str().size()), 0, from);\n",
    "    }\n",
    "    \n",
    "    float SimulateLoRaWAN(uint32_t action) {\n",
    "        // Perform LoRaWAN simulation based on the action (SF, TP)\n",
    "        // Return PDR as a float\n",
    "        return 0.9;  // Example value\n",
    "    }\n",
    "    \n",
    "    float CalculateEnergyConsumption(uint32_t action) {\n",
    "        // Calculate energy consumption based on the action (SF, TP)\n",
    "        return 5.0;  // Example value\n",
    "    }\n",
    "};\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    LoRaWANServer server;\n",
    "    server.RunServer(12345);  // Run server on port 12345\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### **Key Points:**\n",
    "- The NS3 server listens for incoming connections on a **UDP socket**.\n",
    "- When it receives an action from the Python trainer (e.g., SF and TP values), it simulates the LoRaWAN network.\n",
    "- It calculates **PDR** and **energy consumption**, then sends these values back to the Python trainer as a response.\n",
    "\n",
    "### **2. Python Side (Trainer)**\n",
    "In Python, you will create a **client** that sends actions to the NS3 server and receives the rewards. You can use **socket programming** to send and receive messages between Python and NS3.\n",
    "\n",
    "#### **Python Client Example**\n",
    "The Python client sends an action (SF and TP) to the NS3 server and receives the reward (PDR and energy consumption).\n",
    "\n",
    "```python\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "def send_action(action):\n",
    "    # Set up the socket connection\n",
    "    ns3_ip = \"127.0.0.1\"  # Localhost\n",
    "    ns3_port = 12345       # Port used by NS3 server\n",
    "    \n",
    "    # Create a UDP socket\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    \n",
    "    # Send the action to the NS3 server (action: SF and TP)\n",
    "    sock.sendto(struct.pack(\"I\", action), (ns3_ip, ns3_port))\n",
    "    \n",
    "    # Receive the reward (PDR, energy consumption)\n",
    "    data, _ = sock.recvfrom(1024)\n",
    "    \n",
    "    # Unpack the received data (PDR, energy consumption)\n",
    "    pdr, energy_consumption = map(float, data.decode().split(','))\n",
    "    \n",
    "    # Close the socket\n",
    "    sock.close()\n",
    "    \n",
    "    return pdr, energy_consumption\n",
    "\n",
    "# Example: Action (0 = SF=7, TP=2 dBm)\n",
    "action = 0\n",
    "pdr, energy = send_action(action)\n",
    "\n",
    "print(f\"PDR: {pdr}, Energy Consumption: {energy}\")\n",
    "```\n",
    "\n",
    "### **Key Points:**\n",
    "- The Python client sends an **action** (SF and TP) to the NS3 server using a UDP socket.\n",
    "- It waits for a response, which contains the **PDR** and **energy consumption**.\n",
    "- The client unpacks and processes the rewards.\n",
    "\n",
    "### **3. Real-Time Interaction Between Python and NS3**\n",
    "Now, your **Python trainer** can interact with **NS3 in real-time**:\n",
    "- **Python** sends **actions** (SF and TP).\n",
    "- **NS3** simulates the network and sends back the **rewards** (PDR and energy consumption).\n",
    "- The Python agent can use these rewards to train its model using **PPO** or another RL algorithm.\n",
    "\n",
    "### **4. Additional Considerations**\n",
    "- **Latency**: Sockets have inherent latency, so ensure the simulation in NS3 can respond quickly enough.\n",
    "- **Threading**: NS3 and Python can run in parallel, but make sure you handle socket communication efficiently. Consider using multi-threading for real-time interaction if needed.\n",
    "- **Data Conversion**: Ensure that data (like SF, TP, PDR) is correctly encoded and decoded when passing between NS3 and Python.\n",
    "- **Error Handling**: Add error handling for socket communication issues, especially in case of network failures.\n",
    "\n",
    "### **5. Next Steps**\n",
    "- **Integrate the trainer** with your NS3 LoRaWAN simulation, so the action sent by Python directly controls the SF and TP settings in the LoRaWAN model.\n",
    "- **Train your PPO model** by sending actions from the trainer, collecting rewards from NS3, and updating the model iteratively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Would you like more help with any part of the code or need assistance with setting up the NS3 simulation side?** Let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
